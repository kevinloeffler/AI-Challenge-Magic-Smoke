{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11afed6e-38ec-4dde-9b76-aa5343ac1d7e",
   "metadata": {},
   "source": [
    "# Training mit Korrektur für Export\n",
    "\n",
    "Zur Vollständigkeit hier nochmals das ganze Notebook, einfach erst unten mit der Korrektur.\n",
    "\n",
    "Siehe [Errata:-Export-muss-anders-sein-als-bisher](#Errata:-Export-muss-anders-sein-als-bisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae75c1-c131-4f74-8976-bdf9846702bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.foundation import L\n",
    "from fastcore.xtras import Path  # @patch'd properties to the Pathlib module\n",
    "\n",
    "from fastai.callback.schedule import fit_one_cycle, lr_find \n",
    "\n",
    "from fastai.data.block import CategoryBlock, DataBlock\n",
    "from fastai.data.transforms import get_image_files, RandomSplitter\n",
    "\n",
    "from fastai.learner import Learner\n",
    "\n",
    "from fastai.metrics import error_rate\n",
    "from torchvision.models.resnet import resnet34, resnet18\n",
    "import torchvision.models as torch_models\n",
    "\n",
    "\n",
    "from fastai.vision.all import (\n",
    "    aug_transforms,\n",
    "    ImageBlock,\n",
    "    RegressionBlock,\n",
    "    vision_learner,\n",
    "    PILImage,\n",
    ")\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3ae95-d44f-4e68-a635-e358604f1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path('data/sluggish-cheetah')\n",
    "image_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14ac6d-0a46-4530-8da2-5aa9f633a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_json(image):\n",
    "    with open(image_path/f'{image.stem}.json') as f:\n",
    "        json_file = json.load(f)\n",
    "        return[float(json_file['throttle']), float(json_file['steering'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf73d1-ff45-4564-880e-fe8af9eafd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PILImage.create(next(image_path.glob('*.png')))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5763e-d5ec-4e55-9d61-2cdb29f0f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_from_json(next(image_path.glob('*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e935563-92bc-4b4a-ada6-e30e041929d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_block = DataBlock(\n",
    "    blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "    get_items=get_image_files,\n",
    "    splitter=RandomSplitter(),\n",
    "    get_y=read_data_from_json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6789e5-c1b2-4438-8286-8709a1334bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = data_block.dataloaders(Path(image_path), bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d6b72-bf44-4a6e-a8bb-2efee0963a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f99f9-9f50-4038-9636-367e0b3c705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(data_loaders, resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf34214-5e95-4302-8332-0fabdc9fb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1658d21-21a6-405c-bdaf-c782858b3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c808fd5-e966-45ab-8656-e96dbe4361ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(25, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b674ec-08ad-4cb7-b9f7-885c764090ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(ds_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb202bf2-8fee-43f2-a75e-81e5dc6b926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(next(image_path.glob('*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b75a6-2ff6-4eb7-8ec0-271c1f277bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = get_image_files(image_path)\n",
    "test_dl = learn.dls.test_dl(files)\n",
    "preds = learn.get_preds(dl=test_dl)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2d88e-434c-4224-bf62-5a8b317670cc",
   "metadata": {},
   "source": [
    "# Errata: Export muss anders sein als bisher\n",
    "\n",
    "Neu müssen wir via ONNX gehen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2809e7-20af-47ac-82b7-e6a190580e81",
   "metadata": {},
   "source": [
    "## ONNX installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bdb96b-cdc5-44f4-ae19-99e6b20c7d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# onnx installieren\n",
    "!pip install onnx --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc51a2-e498-4207-9dac-54f33bc20042",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unsere Daten exportieren\n",
    "\n",
    "* Der Pfad `models/example.onnx` ist was angepasst werden sollte, sonst wird der potentiell überschrieben.\n",
    "* `input_names=[\"image\"]` und `output_names=[\"data\"]` müssen so heissen und dürfen nicht umbenannt werden. Kleine Frage: Warum könnte das sein?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca20a2e-1688-43ad-b905-7262dcbd6c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "model = learn.model\n",
    "# Modell auf eval setzen, damit die Gewichte korrekt exportiert werden!\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# wir brauchen ein Beispiel Bild um den Input für ONNX festzulegen\n",
    "convert_tensor = transforms.ToTensor()\n",
    "with Image.open(next(image_path.glob('*.png'))) as img:\n",
    "    # wir müssen das Bild in das richtige Format bringen, das\n",
    "    # übernimmt fastai sonst für uns, onnx kennt aber fastai nicht, nur unser model.\n",
    "    tensor_img = convert_tensor(img).unsqueeze(0)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    tensor_img.cuda(),\n",
    "    \"models/example.onnx\",\n",
    "    input_names=[\"image\"],\n",
    "    output_names=[\"data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6b654-30fa-4340-ac1c-ac61bdba26cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exportiertes Modell auf Korrektheit prüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd02a1-da82-433a-a9b9-03a287df055b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensure correctness\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"models/example.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa074c0-6334-4f12-8dcb-019364ccb2b9",
   "metadata": {},
   "source": [
    "## Durchspielen, was auf unserem Fahrzeug passieren wird\n",
    "\n",
    "Dafür braucht es *nur* onnxruntime (`ort`) und pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb00a6-2858-430d-8bbb-0b8ce03eb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnxruntime installieren\n",
    "!pip install onnxruntime --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6b04d-e171-4c7a-8aaf-988f3dd70c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "import numpy\n",
    "\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "def get_inference(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        # wir müssen das Bild in das richtige Format bringen, das\n",
    "        # übernimmt fastai sonst für uns, onnx kennt aber fastai nicht, nur unser model.\n",
    "        tensor_img = convert_tensor(img).unsqueeze(0).numpy()\n",
    "    \n",
    "    ort_sess = ort.InferenceSession('models/example.onnx')\n",
    "    outputs = ort_sess.run(None, {'image': tensor_img})\n",
    "    steering, throttle = outputs[0][0]\n",
    "    return steering, throttle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6ec97-96f8-4faf-b694-266b84fc578c",
   "metadata": {},
   "source": [
    "Tut das auch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef8ecd-0cfc-4a4e-9202-fddd689099fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir nehmen ein einzelnes Bild von uns, auf dem Fahrzeug kommt dies vom Stream\n",
    "example_image_path = next(Path('data/sluggish-cheetah').glob('*.png'))\n",
    "\n",
    "steering, throttle = get_inference(example_image_path)\n",
    "steering, throttle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65f448-2abd-4de4-9131-f3a7433687d0",
   "metadata": {},
   "source": [
    "### Wer mag, kann das auch mit ein paar Bildern prüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef070d-0d42-4646-9723-7ab858a93d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_iterator = Path('data/sluggish-cheetah').glob('*.png')\n",
    "\n",
    "# Annahme: es gibt (mindestens) 20 Bilder\n",
    "for _ in range(20):\n",
    "    im = next(images_iterator)\n",
    "    steering, throttle = get_inference(im)\n",
    "    print(steering, throttle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb2731-e22e-4292-a55f-854583ec81a2",
   "metadata": {},
   "source": [
    "# Auf das Fahrzeug übertragen\n",
    "\n",
    "**Wenn das funktioniert hat**, können wir das models/example.onnx herunterladen und auf dem Fahrzeug an zu den uploads kopieren (zB. `uploads/models/2022-11-11_round-course.onnx`) und dann `SELF_DRIVING_MODEL_PATH` im docker-compose.yml anpassen (zB. `SELF_DRIVING_MODEL_PATH: \"uploads/models/2022-11-11_round-course.onnx\"`) und `docker compose up -d` ausführen.\n",
    "Wenn der Learner importiert wurde, kann dieser wie vorhin weiter verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379572f1-2be5-4719-9a03-391fccf1c66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
